{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Large Language Model on Amazon SageMaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates how to fine-tune a large language model (hypothetically LLaMA 3) on Amazon SageMaker using the Hugging Face Transformers library and Fully Sharded Data Parallel (FSDP).\n",
    "\n",
    "## Contents\n",
    " 1. Setup\n",
    " 2. Set SageMaker session and execution role\n",
    " 3. Configure hyperparameters\n",
    " 4. Prepare training script\n",
    " 5. Configure and launch training job\n",
    " 6. Monitor and analyze results\n",
    "\n",
    "### Note: This notebook assumes you have the necessary permissions to access SageMaker resources and the model.\n",
    "\n",
    "## 1. Setup\n",
    "### Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q U sagemaker transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "#from sagemaker.tensorboard import TensorBoardCallback\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.interactive_apps import tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set SageMaker session and execution role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 00:25:53] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 00:25:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=111588;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=165382;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=31149;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=657145;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/10/24 00:25:54] </span><span style=\"color: #d7af00; text-decoration-color: #d7af00; font-weight: bold\">WARNING </span> Couldn't call <span style=\"color: #008700; text-decoration-color: #008700\">'get_role'</span> to get Role ARN from role name dev_desktop to <a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/sagemaker/session.py#5971\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">5971</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         get Role path.                                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/10/24 00:25:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;175;0mWARNING \u001b[0m Couldn't call \u001b[38;2;0;135;0m'get_role'\u001b[0m to get Role ARN from role name dev_desktop to \u001b]8;id=902716;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=874898;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/sagemaker/session.py#5971\u001b\\\u001b[2m5971\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         get Role path.                                                         \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials in shared credentials file: ~<span style=\"color: #e100e1; text-decoration-color: #e100e1\">/.aws/credentials</span>   <a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1278</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials in shared credentials file: ~\u001b[38;2;225;0;225m/.aws/\u001b[0m\u001b[38;2;225;0;225mcredentials\u001b[0m   \u001b]8;id=355847;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=274850;file:///home/rivasge/projects/Meta-Llama-on-AWS/Llama3-finetuning-for-Bedrock/.venv/lib/python3.10/site-packages/botocore/credentials.py#1278\u001b\\\u001b[2m1278\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::786045444066:role/service-role/AmazonSageMaker-ExecutionRole-20220929T161862\n",
      "sagemaker bucket: sagemaker-us-west-2-786045444066\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    #role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "    #use this code if you are running locally\n",
    "    role = iam.get_role(RoleName='AmazonSageMaker-ExecutionRole-20220929T161862')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client('sagemaker', region_name=sess.boto_region_name)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare and upload dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section demonstrates how to prepare your dataset locally and upload it to S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare your dataset\n",
    "Assuming you have a CSV file with your data. Adjust as needed for your data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a CSV file with your data. Adjust as needed for your data format.\n",
    "df = pd.read_csv('path/to/your/local/dataset.csv')\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "print(f\"Train set size: {len(dataset['train'])}\")\n",
    "print(f\"Validation set size: {len(dataset['test'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for upload\n",
    "Create temporary files for train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'].to_json('train.jsonl', orient='records', lines=True)\n",
    "dataset['test'].to_json('val.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'your-s3-bucket-name'\n",
    "train_key = 'path/in/bucket/train.jsonl'\n",
    "val_key = 'path/in/bucket/val.jsonl'\n",
    "\n",
    "# Upload train set\n",
    "s3_client.upload_file('train.jsonl', bucket_name, train_key)\n",
    "\n",
    "# Upload validation set\n",
    "s3_client.upload_file('val.jsonl', bucket_name, val_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set S3 paths for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = f's3://{bucket_name}/{train_key}'\n",
    "eval_data_path = f's3://{bucket_name}/{val_key}'\n",
    "\n",
    "print(f\"Training data uploaded to: {train_data_path}\")\n",
    "print(f\"Validation data uploaded to: {eval_data_path}\")\n",
    "\n",
    "# Clean up local temporary files\n",
    "os.remove('train.jsonl')\n",
    "os.remove('val.jsonl')\n",
    "\n",
    "print(\"Local temporary files cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use train_data_path and eval_data_path in your SageMaker estimator fit() method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Configure hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "  model_name_or_path: meta-llama/Llama-3.3-70b\n",
      "  output_dir: /opt/ml/model\n",
      "  num_train_epochs: 3\n",
      "  per_device_train_batch_size: 1\n",
      "  per_device_eval_batch_size: 1\n",
      "  warmup_steps: 500\n",
      "  weight_decay: 0.01\n",
      "  logging_dir: /opt/ml/output/logs\n",
      "  logging_steps: 10\n",
      "  evaluation_strategy: steps\n",
      "  eval_steps: 500\n",
      "  save_steps: 1000\n",
      "  save_total_limit: 3\n",
      "  fp16: True\n",
      "  fsdp: full_shard auto_wrap\n",
      "  fsdp_transformer_layer_cls_to_wrap: LlamaDecoderLayer\n"
     ]
    }
   ],
   "source": [
    "hyperparameters = {\n",
    "    'model_name_or_path': 'meta-llama/Llama-3.3-70b',  # Hypothetical model name\n",
    "    'output_dir': '/opt/ml/model',\n",
    "    'dataset_name': 'wikitext',\n",
    "    'dataset_config_name': 'wikitext-2-raw-v1',\n",
    "    'max_train_samples': 1000,  # Adjust as needed\n",
    "    'max_eval_samples': 100,  # Adjust as needed\n",
    "    'block_size': 1024,\n",
    "}\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "for key, value in hyperparameters.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for our training script\n",
    "!mkdir -p scripts\n",
    "#Create a subdirectory for training scripts\n",
    "!mkdir -p scripts/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/training/train_llama_33_70b_fsdp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/training/train_llama_33_70b_fsdp.py\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel as FSDP,\n",
    "    MixedPrecision,\n",
    "    BackwardPrefetch,\n",
    "    ShardingStrategy,\n",
    "    CPUOffload,\n",
    ")\n",
    "from torch.distributed.fsdp.wrap import (\n",
    "    transformer_auto_wrap_policy,\n",
    "    size_based_auto_wrap_policy,\n",
    "    enable_wrap,\n",
    "    wrap,\n",
    ")\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name_or_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"wikitext\")\n",
    "    parser.add_argument(\"--dataset_config_name\", type=str, default=\"wikitext-2-raw-v1\")\n",
    "    parser.add_argument(\"--max_train_samples\", type=int, default=None)\n",
    "    parser.add_argument(\"--max_eval_samples\", type=int, default=None)\n",
    "    parser.add_argument(\"--block_size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"/opt/ml/model\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def setup_fsdp(model):\n",
    "    \"\"\"\n",
    "    Set up Fully Sharded Data Parallel (FSDP) for the model.\n",
    "    \"\"\"\n",
    "    from transformers.models.llama.modeling_llama import LlamaDecoderLayer\n",
    "\n",
    "    torch.distributed.init_process_group(backend=\"nccl\")\n",
    "    \n",
    "    mixed_precision_policy = MixedPrecision(\n",
    "        param_dtype=torch.float16,\n",
    "        reduce_dtype=torch.float16,\n",
    "        buffer_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    fsdp_model = FSDP(\n",
    "        model,\n",
    "        sharding_strategy=ShardingStrategy.FULL_SHARD,\n",
    "        mixed_precision=mixed_precision_policy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        auto_wrap_policy=transformer_auto_wrap_policy(transformer_layer_cls={LlamaDecoderLayer}),\n",
    "        backward_prefetch=BackwardPrefetch.BACKWARD_PRE,\n",
    "        cpu_offload=CPUOffload(offload_params=True),\n",
    "    )\n",
    "    \n",
    "    return fsdp_model\n",
    "\n",
    "def main():\n",
    "    args = parse_args()\n",
    "    \n",
    "    # SageMaker specific: Set the output directory\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=1,  # Reduced batch size for FSDP\n",
    "        per_device_eval_batch_size=1,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='/opt/ml/output/tensorboard',\n",
    "        logging_steps=10,\n",
    "        fp16=True,  # Enable mixed precision training\n",
    "        gradient_checkpointing=True,  # Enable gradient checkpointing\n",
    "        dataloader_num_workers=4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=3,\n",
    "    )\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    set_seed(training_args.seed)\n",
    "\n",
    "    # Load pretrained model and tokenizer\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name_or_path,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "\n",
    "    # Set up FSDP\n",
    "    model = setup_fsdp(model)\n",
    "\n",
    "    # Load and preprocess the dataset\n",
    "    raw_datasets = load_dataset(args.dataset_name, args.dataset_config_name)\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, max_length=args.block_size)\n",
    "\n",
    "    tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    if args.max_train_samples:\n",
    "        train_dataset = train_dataset.select(range(args.max_train_samples))\n",
    "\n",
    "    eval_dataset = tokenized_datasets[\"validation\"]\n",
    "    if args.max_eval_samples:\n",
    "        eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
    "\n",
    "    # Initialize our Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    train_result = trainer.train()\n",
    "    trainer.save_model()\n",
    "    trainer.log_metrics(\"train\", train_result.metrics)\n",
    "    trainer.save_metrics(\"train\", train_result.metrics)\n",
    "\n",
    "    # Evaluation\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "print(\"Training script created at scripts/train.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Configure and launch training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='training/train_llama_33_70b_fsdp.py',\n",
    "    source_dir='./scripts',\n",
    "    instance_type='ml.p4d.24xlarge',\n",
    "    instance_count=2,\n",
    "    role=role,\n",
    "    transformers_version='4.28',\n",
    "    pytorch_version='2.0',\n",
    "    py_version='py39',\n",
    "    hyperparameters=hyperparameters,\n",
    "    distribution={\n",
    "        'torch_distributed': {\n",
    "            'enabled': True\n",
    "        }\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 locations for train and evaluation data\n",
    "train_data_path = 's3://your-bucket/path/to/train/data'\n",
    "eval_data_path = 's3://your-bucket/path/to/eval/data'\n",
    "\n",
    "print(f\"Training data S3 location: {train_data_path}\")\n",
    "print(f\"Evaluation data S3 location: {eval_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the training job\n",
    "huggingface_estimator.fit({\n",
    "    'train': train_data_path,\n",
    "    'eval': eval_data_path\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Monitor and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the name of the training job\n",
    "training_job_name = huggingface_estimator.latest_training_job.job_name\n",
    "print(f\"Training job name: {training_job_name}\")\n",
    "\n",
    "# You can use the SageMaker console to monitor the progress of your training job\n",
    "print(f\"Monitor your training job at: https://{sagemaker_session.boto_region_name}.console.aws.amazon.com/sagemaker/home?region={sagemaker_session.boto_region_name}#/jobs/{training_job_name}\")\n",
    "\n",
    "# After training is complete, you can analyze the results\n",
    "# This might include evaluating the model, reviewing logs, or examining saved checkpoints\n",
    "\n",
    "print(\"Training complete. Review the SageMaker console for detailed logs and results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
